{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\W_software\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches.meta', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'readme.html', 'test_batch']\n"
     ]
    }
   ],
   "source": [
    "CAFIR_DIR = \"./cifar-10-batches-py/\"\n",
    "print(os.listdir(CAFIR_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./cifar-10-batches-py/data_batch_1', './cifar-10-batches-py/data_batch_2', './cifar-10-batches-py/data_batch_3', './cifar-10-batches-py/data_batch_4', './cifar-10-batches-py/data_batch_5']\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "(2000, 3072)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "def load_file(filename):\n",
    "    '''read file return date and lable'''\n",
    "    with open(filename,'rb') as f:\n",
    "        data = pickle.load(f,encoding=\"bytes\")\n",
    "        return data[b'data'],data[b'labels']\n",
    "#c创建读取数据的类，需要参数filename,isTrain,shuffle\n",
    "class CafirDate:\n",
    "    #初始化数据\n",
    "    def __init__(self,filenames,isShuffle):\n",
    "        #存放数据\n",
    "        datas = []\n",
    "        #存放标签\n",
    "        labels = []\n",
    "        for filename in filenames:\n",
    "            data,label = load_file(filename)\n",
    "            #测试二分类，只对【0,1】标签的数据进行存取\n",
    "            for item,index in zip(data,label):\n",
    "                if index in [0,1]:\n",
    "                    datas.append(item)\n",
    "                    labels.append(index)\n",
    "        #对得到的数据纵向进行拼接\n",
    "        self.datas = np.vstack(datas)\n",
    "        self.datas = self.datas / 127.5 - 1\n",
    "        #对得到的标签进行横向拼接\n",
    "        self.labels = np.hstack(labels)\n",
    "        \n",
    "        \n",
    "        #测试\n",
    "        print(self.datas.shape)\n",
    "        print(self.labels.shape)\n",
    "        \n",
    "        \n",
    "        #获取共有多少条数据\n",
    "        self.num_examples = self.datas.shape[0]\n",
    "        #获取是否需要打乱数据\n",
    "        self.isShuffle = isShuffle\n",
    "        #设置起始位置，batch_size 时使用\n",
    "        self.indicator = 0\n",
    "        #先打乱下顺序\n",
    "        if self.isShuffle:\n",
    "            self.Shuffle()\n",
    "    def Shuffle(self):\n",
    "        #随机的打乱标签\n",
    "        index = np.random.permutation(self.num_examples)\n",
    "        self.datas = self.datas[index]\n",
    "        self.labels = self.labels[index]\n",
    "    def next_batch(self,batch_size):\n",
    "        '''return batch_size example '''\n",
    "        self.endIndicator = self.indicator+batch_size\n",
    "        if self.endIndicator > self.num_examples:\n",
    "            #如果是训练过程，就可以重新取值\n",
    "            if self.isShuffle:\n",
    "                #首先重新打乱\n",
    "                self.Shuffle()\n",
    "                self.indicator = 0\n",
    "                self.endIndicator = self.indicator+batch_size\n",
    "            else:\n",
    "                raise Exception('No More data..')\n",
    "        #如果还是大于总数\n",
    "        if self.endIndicator > self.num_examples:\n",
    "            raise Exception('batch_size too lagre...')\n",
    "        batch_data = self.datas[self.indicator:self.endIndicator]\n",
    "        batch_label = self.labels[self.indicator:self.endIndicator]\n",
    "        self.indicator = self.endIndicator\n",
    "        return batch_data,batch_label\n",
    "    \n",
    "    \n",
    "#得到训练数据集的名字\n",
    "filenames = [CAFIR_DIR+\"data_batch_%d\" %i for i in range(1,6)]\n",
    "print(filenames)\n",
    "#得到训练数据集的对象\n",
    "train_data  =  CafirDate(filenames,True)  \n",
    "\n",
    "\n",
    "#获取测试数据集的名字\n",
    "testFilenames = [CAFIR_DIR+\"test_batch\"]\n",
    "test_data = CafirDate(testFilenames,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置变量\n",
    "x = tf.placeholder(tf.float32,[None,3072])\n",
    "y = tf.placeholder(tf.int32,[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化参数\n",
    "w = tf.get_variable('w',[x.get_shape()[-1],1],initializer = tf.random_normal_initializer(0,0.1))\n",
    "b = tf.get_variable('b',[1],initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建神经网络  \n",
    "#[None,1]\n",
    "y_ = tf.matmul(x,w)+b\n",
    "y_ = tf.nn.sigmoid(y_)\n",
    "#[None,1]\n",
    "y_reshape = tf.reshape(y,(-1,1))\n",
    "#类型\n",
    "y_reshape_float = tf.cast(y_reshape,tf.float32)\n",
    "#创建损失函数\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y_reshape_float-y_))\n",
    "\n",
    "#计算准确率\n",
    "# >0.5 的为预测准确的\n",
    "y_pre = y_ > 0.5\n",
    "y_pre = tf.cast(y_pre,tf.int32)  \n",
    "#进行比较  [1,0,0,0,0,1,1,0,0,.....]     ！！！！！！！！！！！！！！！！！判断类型的时候一定要保证shape是一致的。！！！！！！\n",
    "y_predict = tf.equal(y_pre,y_reshape)\n",
    "#计算准确率\n",
    "acc = tf.reduce_mean(tf.cast(y_predict,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]step:500  _____train_loss:0.189450 ____train_acc:0.75000\n",
      "[train]step:1000  _____train_loss:0.219524 ____train_acc:0.75000\n",
      "[train]step:1500  _____train_loss:0.151091 ____train_acc:0.75000\n",
      "[train]step:2000  _____train_loss:0.083754 ____train_acc:0.90000\n",
      "[train]step:2500  _____train_loss:0.051658 ____train_acc:0.95000\n",
      "[train]step:3000  _____train_loss:0.078257 ____train_acc:0.95000\n",
      "[train]step:3500  _____train_loss:0.084973 ____train_acc:0.85000\n",
      "[train]step:4000  _____train_loss:0.162157 ____train_acc:0.75000\n",
      "[train]step:4500  _____train_loss:0.239582 ____train_acc:0.65000\n",
      "[train]step:5000  _____train_loss:0.118424 ____train_acc:0.85000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:5000____acc:0.81700  \n",
      "[train]step:5500  _____train_loss:0.132844 ____train_acc:0.90000\n",
      "[train]step:6000  _____train_loss:0.121220 ____train_acc:0.80000\n",
      "[train]step:6500  _____train_loss:0.104202 ____train_acc:0.85000\n",
      "[train]step:7000  _____train_loss:0.235452 ____train_acc:0.70000\n",
      "[train]step:7500  _____train_loss:0.110387 ____train_acc:0.85000\n",
      "[train]step:8000  _____train_loss:0.097682 ____train_acc:0.85000\n",
      "[train]step:8500  _____train_loss:0.046320 ____train_acc:0.95000\n",
      "[train]step:9000  _____train_loss:0.094123 ____train_acc:0.90000\n",
      "[train]step:9500  _____train_loss:0.060768 ____train_acc:0.90000\n",
      "[train]step:10000  _____train_loss:0.061908 ____train_acc:0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:10000____acc:0.81400  \n",
      "[train]step:10500  _____train_loss:0.145762 ____train_acc:0.80000\n",
      "[train]step:11000  _____train_loss:0.225542 ____train_acc:0.70000\n",
      "[train]step:11500  _____train_loss:0.144660 ____train_acc:0.85000\n",
      "[train]step:12000  _____train_loss:0.118437 ____train_acc:0.85000\n",
      "[train]step:12500  _____train_loss:0.069300 ____train_acc:0.95000\n",
      "[train]step:13000  _____train_loss:0.078886 ____train_acc:0.90000\n",
      "[train]step:13500  _____train_loss:0.088037 ____train_acc:0.90000\n",
      "[train]step:14000  _____train_loss:0.066921 ____train_acc:0.90000\n",
      "[train]step:14500  _____train_loss:0.083731 ____train_acc:0.85000\n",
      "[train]step:15000  _____train_loss:0.203927 ____train_acc:0.70000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:15000____acc:0.80850  \n",
      "[train]step:15500  _____train_loss:0.200345 ____train_acc:0.75000\n",
      "[train]step:16000  _____train_loss:0.118704 ____train_acc:0.85000\n",
      "[train]step:16500  _____train_loss:0.202172 ____train_acc:0.75000\n",
      "[train]step:17000  _____train_loss:0.051382 ____train_acc:0.90000\n",
      "[train]step:17500  _____train_loss:0.030070 ____train_acc:1.00000\n",
      "[train]step:18000  _____train_loss:0.149576 ____train_acc:0.80000\n",
      "[train]step:18500  _____train_loss:0.152850 ____train_acc:0.80000\n",
      "[train]step:19000  _____train_loss:0.061763 ____train_acc:0.95000\n",
      "[train]step:19500  _____train_loss:0.086852 ____train_acc:0.90000\n",
      "[train]step:20000  _____train_loss:0.184241 ____train_acc:0.75000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:20000____acc:0.81300  \n",
      "[train]step:20500  _____train_loss:0.139160 ____train_acc:0.80000\n",
      "[train]step:21000  _____train_loss:0.176057 ____train_acc:0.75000\n",
      "[train]step:21500  _____train_loss:0.149601 ____train_acc:0.85000\n",
      "[train]step:22000  _____train_loss:0.215521 ____train_acc:0.75000\n",
      "[train]step:22500  _____train_loss:0.110090 ____train_acc:0.80000\n",
      "[train]step:23000  _____train_loss:0.154068 ____train_acc:0.85000\n",
      "[train]step:23500  _____train_loss:0.124566 ____train_acc:0.80000\n",
      "[train]step:24000  _____train_loss:0.215719 ____train_acc:0.75000\n",
      "[train]step:24500  _____train_loss:0.039832 ____train_acc:0.95000\n",
      "[train]step:25000  _____train_loss:0.123659 ____train_acc:0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:25000____acc:0.81500  \n",
      "[train]step:25500  _____train_loss:0.068829 ____train_acc:0.95000\n",
      "[train]step:26000  _____train_loss:0.148126 ____train_acc:0.80000\n",
      "[train]step:26500  _____train_loss:0.099541 ____train_acc:0.85000\n",
      "[train]step:27000  _____train_loss:0.178735 ____train_acc:0.85000\n",
      "[train]step:27500  _____train_loss:0.096521 ____train_acc:0.85000\n",
      "[train]step:28000  _____train_loss:0.003124 ____train_acc:1.00000\n",
      "[train]step:28500  _____train_loss:0.167551 ____train_acc:0.80000\n",
      "[train]step:29000  _____train_loss:0.132391 ____train_acc:0.75000\n",
      "[train]step:29500  _____train_loss:0.159740 ____train_acc:0.80000\n",
      "[train]step:30000  _____train_loss:0.098105 ____train_acc:0.85000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:30000____acc:0.80600  \n",
      "[train]step:30500  _____train_loss:0.039648 ____train_acc:0.95000\n",
      "[train]step:31000  _____train_loss:0.024066 ____train_acc:0.95000\n",
      "[train]step:31500  _____train_loss:0.091879 ____train_acc:0.90000\n",
      "[train]step:32000  _____train_loss:0.136168 ____train_acc:0.85000\n",
      "[train]step:32500  _____train_loss:0.104571 ____train_acc:0.85000\n",
      "[train]step:33000  _____train_loss:0.189343 ____train_acc:0.80000\n",
      "[train]step:33500  _____train_loss:0.053595 ____train_acc:0.95000\n",
      "[train]step:34000  _____train_loss:0.202826 ____train_acc:0.65000\n",
      "[train]step:34500  _____train_loss:0.182589 ____train_acc:0.80000\n",
      "[train]step:35000  _____train_loss:0.024733 ____train_acc:1.00000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:35000____acc:0.81400  \n",
      "[train]step:35500  _____train_loss:0.083359 ____train_acc:0.90000\n",
      "[train]step:36000  _____train_loss:0.081035 ____train_acc:0.85000\n",
      "[train]step:36500  _____train_loss:0.168314 ____train_acc:0.80000\n",
      "[train]step:37000  _____train_loss:0.115292 ____train_acc:0.85000\n",
      "[train]step:37500  _____train_loss:0.165418 ____train_acc:0.80000\n",
      "[train]step:38000  _____train_loss:0.041386 ____train_acc:0.95000\n",
      "[train]step:38500  _____train_loss:0.193084 ____train_acc:0.80000\n",
      "[train]step:39000  _____train_loss:0.090221 ____train_acc:0.90000\n",
      "[train]step:39500  _____train_loss:0.021947 ____train_acc:1.00000\n",
      "[train]step:40000  _____train_loss:0.188360 ____train_acc:0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:40000____acc:0.80200  \n",
      "[train]step:40500  _____train_loss:0.074950 ____train_acc:0.90000\n",
      "[train]step:41000  _____train_loss:0.108389 ____train_acc:0.80000\n",
      "[train]step:41500  _____train_loss:0.075522 ____train_acc:0.85000\n",
      "[train]step:42000  _____train_loss:0.060393 ____train_acc:0.85000\n",
      "[train]step:42500  _____train_loss:0.164678 ____train_acc:0.75000\n",
      "[train]step:43000  _____train_loss:0.225956 ____train_acc:0.75000\n",
      "[train]step:43500  _____train_loss:0.141560 ____train_acc:0.85000\n",
      "[train]step:44000  _____train_loss:0.223600 ____train_acc:0.70000\n",
      "[train]step:44500  _____train_loss:0.089035 ____train_acc:0.90000\n",
      "[train]step:45000  _____train_loss:0.142690 ____train_acc:0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:45000____acc:0.80950  \n",
      "[train]step:45500  _____train_loss:0.136942 ____train_acc:0.85000\n",
      "[train]step:46000  _____train_loss:0.085385 ____train_acc:0.90000\n",
      "[train]step:46500  _____train_loss:0.028939 ____train_acc:0.95000\n",
      "[train]step:47000  _____train_loss:0.102632 ____train_acc:0.85000\n",
      "[train]step:47500  _____train_loss:0.137119 ____train_acc:0.75000\n",
      "[train]step:48000  _____train_loss:0.063069 ____train_acc:0.90000\n",
      "[train]step:48500  _____train_loss:0.069485 ____train_acc:0.95000\n",
      "[train]step:49000  _____train_loss:0.069729 ____train_acc:0.90000\n",
      "[train]step:49500  _____train_loss:0.187578 ____train_acc:0.75000\n",
      "[train]step:50000  _____train_loss:0.077250 ____train_acc:0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:50000____acc:0.81800  \n",
      "[train]step:50500  _____train_loss:0.180459 ____train_acc:0.80000\n",
      "[train]step:51000  _____train_loss:0.102220 ____train_acc:0.90000\n",
      "[train]step:51500  _____train_loss:0.022555 ____train_acc:1.00000\n",
      "[train]step:52000  _____train_loss:0.102031 ____train_acc:0.85000\n",
      "[train]step:52500  _____train_loss:0.044344 ____train_acc:0.95000\n",
      "[train]step:53000  _____train_loss:0.010623 ____train_acc:1.00000\n",
      "[train]step:53500  _____train_loss:0.155254 ____train_acc:0.75000\n",
      "[train]step:54000  _____train_loss:0.042291 ____train_acc:0.95000\n",
      "[train]step:54500  _____train_loss:0.098148 ____train_acc:0.90000\n",
      "[train]step:55000  _____train_loss:0.298666 ____train_acc:0.60000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:55000____acc:0.82050  \n",
      "[train]step:55500  _____train_loss:0.117854 ____train_acc:0.85000\n",
      "[train]step:56000  _____train_loss:0.068801 ____train_acc:0.85000\n",
      "[train]step:56500  _____train_loss:0.156889 ____train_acc:0.80000\n",
      "[train]step:57000  _____train_loss:0.083413 ____train_acc:0.85000\n",
      "[train]step:57500  _____train_loss:0.078532 ____train_acc:0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]step:58000  _____train_loss:0.028441 ____train_acc:1.00000\n",
      "[train]step:58500  _____train_loss:0.110926 ____train_acc:0.80000\n",
      "[train]step:59000  _____train_loss:0.134136 ____train_acc:0.85000\n",
      "[train]step:59500  _____train_loss:0.114056 ____train_acc:0.80000\n",
      "[train]step:60000  _____train_loss:0.130963 ____train_acc:0.85000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:60000____acc:0.80200  \n",
      "[train]step:60500  _____train_loss:0.126993 ____train_acc:0.85000\n",
      "[train]step:61000  _____train_loss:0.082265 ____train_acc:0.90000\n",
      "[train]step:61500  _____train_loss:0.010283 ____train_acc:1.00000\n",
      "[train]step:62000  _____train_loss:0.071827 ____train_acc:0.95000\n",
      "[train]step:62500  _____train_loss:0.058678 ____train_acc:0.95000\n",
      "[train]step:63000  _____train_loss:0.013388 ____train_acc:1.00000\n",
      "[train]step:63500  _____train_loss:0.080580 ____train_acc:0.95000\n",
      "[train]step:64000  _____train_loss:0.039383 ____train_acc:0.95000\n",
      "[train]step:64500  _____train_loss:0.019201 ____train_acc:1.00000\n",
      "[train]step:65000  _____train_loss:0.072057 ____train_acc:0.95000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:65000____acc:0.81450  \n",
      "[train]step:65500  _____train_loss:0.065301 ____train_acc:0.85000\n",
      "[train]step:66000  _____train_loss:0.149405 ____train_acc:0.80000\n",
      "[train]step:66500  _____train_loss:0.222293 ____train_acc:0.70000\n",
      "[train]step:67000  _____train_loss:0.146126 ____train_acc:0.85000\n",
      "[train]step:67500  _____train_loss:0.126072 ____train_acc:0.85000\n",
      "[train]step:68000  _____train_loss:0.073207 ____train_acc:0.95000\n",
      "[train]step:68500  _____train_loss:0.071553 ____train_acc:0.90000\n",
      "[train]step:69000  _____train_loss:0.154835 ____train_acc:0.85000\n",
      "[train]step:69500  _____train_loss:0.088022 ____train_acc:0.85000\n",
      "[train]step:70000  _____train_loss:0.076341 ____train_acc:0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:70000____acc:0.80400  \n",
      "[train]step:70500  _____train_loss:0.173397 ____train_acc:0.75000\n",
      "[train]step:71000  _____train_loss:0.208995 ____train_acc:0.75000\n",
      "[train]step:71500  _____train_loss:0.021538 ____train_acc:0.95000\n",
      "[train]step:72000  _____train_loss:0.099393 ____train_acc:0.90000\n",
      "[train]step:72500  _____train_loss:0.164499 ____train_acc:0.80000\n",
      "[train]step:73000  _____train_loss:0.155482 ____train_acc:0.80000\n",
      "[train]step:73500  _____train_loss:0.037652 ____train_acc:0.95000\n",
      "[train]step:74000  _____train_loss:0.094447 ____train_acc:0.85000\n",
      "[train]step:74500  _____train_loss:0.111309 ____train_acc:0.80000\n",
      "[train]step:75000  _____train_loss:0.117345 ____train_acc:0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:75000____acc:0.79600  \n",
      "[train]step:75500  _____train_loss:0.098256 ____train_acc:0.90000\n",
      "[train]step:76000  _____train_loss:0.141384 ____train_acc:0.80000\n",
      "[train]step:76500  _____train_loss:0.071924 ____train_acc:0.95000\n",
      "[train]step:77000  _____train_loss:0.195456 ____train_acc:0.80000\n",
      "[train]step:77500  _____train_loss:0.046652 ____train_acc:0.90000\n",
      "[train]step:78000  _____train_loss:0.051993 ____train_acc:0.95000\n",
      "[train]step:78500  _____train_loss:0.043494 ____train_acc:0.95000\n",
      "[train]step:79000  _____train_loss:0.062429 ____train_acc:0.95000\n",
      "[train]step:79500  _____train_loss:0.079201 ____train_acc:0.90000\n",
      "[train]step:80000  _____train_loss:0.057585 ____train_acc:0.95000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:80000____acc:0.80250  \n",
      "[train]step:80500  _____train_loss:0.199507 ____train_acc:0.80000\n",
      "[train]step:81000  _____train_loss:0.173371 ____train_acc:0.75000\n",
      "[train]step:81500  _____train_loss:0.080943 ____train_acc:0.90000\n",
      "[train]step:82000  _____train_loss:0.103348 ____train_acc:0.90000\n",
      "[train]step:82500  _____train_loss:0.086784 ____train_acc:0.85000\n",
      "[train]step:83000  _____train_loss:0.077866 ____train_acc:0.90000\n",
      "[train]step:83500  _____train_loss:0.012286 ____train_acc:1.00000\n",
      "[train]step:84000  _____train_loss:0.112702 ____train_acc:0.85000\n",
      "[train]step:84500  _____train_loss:0.190843 ____train_acc:0.80000\n",
      "[train]step:85000  _____train_loss:0.103777 ____train_acc:0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:85000____acc:0.78350  \n",
      "[train]step:85500  _____train_loss:0.046288 ____train_acc:0.95000\n",
      "[train]step:86000  _____train_loss:0.110333 ____train_acc:0.90000\n",
      "[train]step:86500  _____train_loss:0.125225 ____train_acc:0.80000\n",
      "[train]step:87000  _____train_loss:0.080695 ____train_acc:0.90000\n",
      "[train]step:87500  _____train_loss:0.143999 ____train_acc:0.85000\n",
      "[train]step:88000  _____train_loss:0.142270 ____train_acc:0.85000\n",
      "[train]step:88500  _____train_loss:0.130378 ____train_acc:0.85000\n",
      "[train]step:89000  _____train_loss:0.147788 ____train_acc:0.80000\n",
      "[train]step:89500  _____train_loss:0.071968 ____train_acc:0.85000\n",
      "[train]step:90000  _____train_loss:0.033094 ____train_acc:0.95000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:90000____acc:0.80900  \n",
      "[train]step:90500  _____train_loss:0.111794 ____train_acc:0.85000\n",
      "[train]step:91000  _____train_loss:0.112534 ____train_acc:0.90000\n",
      "[train]step:91500  _____train_loss:0.189534 ____train_acc:0.75000\n",
      "[train]step:92000  _____train_loss:0.067897 ____train_acc:0.95000\n",
      "[train]step:92500  _____train_loss:0.110200 ____train_acc:0.85000\n",
      "[train]step:93000  _____train_loss:0.063977 ____train_acc:0.90000\n",
      "[train]step:93500  _____train_loss:0.032757 ____train_acc:0.95000\n",
      "[train]step:94000  _____train_loss:0.112082 ____train_acc:0.80000\n",
      "[train]step:94500  _____train_loss:0.056428 ____train_acc:0.95000\n",
      "[train]step:95000  _____train_loss:0.153946 ____train_acc:0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:95000____acc:0.81750  \n",
      "[train]step:95500  _____train_loss:0.067644 ____train_acc:0.85000\n",
      "[train]step:96000  _____train_loss:0.064311 ____train_acc:0.95000\n",
      "[train]step:96500  _____train_loss:0.086054 ____train_acc:0.85000\n",
      "[train]step:97000  _____train_loss:0.009602 ____train_acc:1.00000\n",
      "[train]step:97500  _____train_loss:0.093527 ____train_acc:0.85000\n",
      "[train]step:98000  _____train_loss:0.024834 ____train_acc:1.00000\n",
      "[train]step:98500  _____train_loss:0.094434 ____train_acc:0.85000\n",
      "[train]step:99000  _____train_loss:0.098723 ____train_acc:0.85000\n",
      "[train]step:99500  _____train_loss:0.027235 ____train_acc:0.95000\n",
      "[train]step:100000  _____train_loss:0.081250 ____train_acc:0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[test ] step:100000____acc:0.81450  \n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "#设置batch_size\n",
    "batch_size = 20\n",
    "#设置train_step \n",
    "train_step = 100000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_step):\n",
    "        #首先获取batch_size 数据\n",
    "        batch_data,batch_label = train_data.next_batch(batch_size)\n",
    "        _,loss_,acc_val = sess.run([train_op,loss,acc], feed_dict = {x:batch_data,y:batch_label})\n",
    "        if (i+1)%500 == 0:\n",
    "            print(\"[train]step:%d  _____train_loss:%f ____train_acc:%4.5f\" % (i+1,loss_,acc_val))\n",
    "        if (i+1)%5000 == 0:\n",
    "            test_data = CafirDate(testFilenames,False)\n",
    "            accs = []\n",
    "           \n",
    "            for j in range(100):\n",
    "                test_batch_data,test_batch_label = test_data.next_batch(batch_size)\n",
    "                t_acc_ = sess.run(acc,feed_dict={x:test_batch_data,y:test_batch_label})\n",
    "                accs.append(t_acc_)\n",
    "            test_acc = np.mean(accs)\n",
    "            print(\"[test ] step:%d____acc:%4.5f  \" % (i+1,test_acc))\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
